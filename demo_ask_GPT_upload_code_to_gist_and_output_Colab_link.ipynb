{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_n1Hit7RLLwt",
        "uYuMA4kg3zwn",
        "tFeIqizs-KQ3",
        "ZLeJ_iEHuO8U",
        "1k3u2fF2wn1P",
        "o2NG9J8hmiRf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hNftpeEsHn4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mdai\n",
        "\n",
        "import mdai\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mdai\n",
        "DOMAIN = # your domain\n",
        "YOUR_PERSONAL_TOKEN = # your personal token\n",
        "mdai_client = mdai.Client(domain=DOMAIN, access_token=YOUR_PERSONAL_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbOWMskSvupI",
        "outputId": "4416f08e-4bec-4aa1-99ba-6c32a708f092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully authenticated to staging.md.ai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo1: asks for download annotations then convert to dataframe and store as csv in a given place"
      ],
      "metadata": {
        "id": "_n1Hit7RLLwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Question: write me a script for the whole pipeline that gets a dataframe for annotation and store in my default folder\"\n",
        "report_fixed_part = \"\"\"You are an intelligent assistant helping users with their questions on techniques of MDai cloud platform.\n",
        "There is a set of rules:\n",
        "1. If the context does not answer some parts, and these parts are about coding, use common coding knowledge.\n",
        "2. For code that interacts with MD.ai, strictly follow the function names\n",
        "3. For conceptual questions, do not make up an answer and strictly answer based on the context. If the context is empty, just say \"I do not know the answer.”\n",
        "4. Some of the personal information might not be needed\n",
        "5.After downloading annotation json file, always use ”glob” library to find name. Do not create a random name of the file. Key words to use: \"annotation\", \"dicom_metadata\", \"model_outputs\"\n",
        "6.mdai has a utils module and contains library like common_utils\n",
        "7. MDai download functions mdai_client.project and mdai_client.download_dicom_metadata functions have an optional parameter called \"dataset_id\" , they have to match, such as both = 1234 or both = None.\n",
        "8. Start with \"!pip install\" the library that is not in Google Colab notebook by default\n",
        "9. All generated code will be upload to gist and then output as a colab notebook link\n",
        "10. The mdai authentication function is mdai.Client(domain, access_token), do not do any extra authentication step.\n",
        "\n",
        "tips:\n",
        "The pipeline of exporting dicom sr is 1. download the annotation and metadata json files using the mdai_client.project and mdai_client.download_dicom_metadata functions. Do not download anything else such as model output, do not convert json to dataframe 2. after downloading, find these 2 json files by keyword 'dicom_metadata' and 'annotations', pass them as input to do the DICOM SR export\n",
        "\n",
        "The pipeline of creating label is: download annotations json, convert json to dataframe, create the mapping dictionary\n",
        "\n",
        "For convert Dicom SR to annotations, assume there is already an output folder that contains SR file (so do not download for these tasks) and iterate all files ending with dcm. Assume the SR_to_Annot function is already in library mdai.utils.common_utils\n",
        "\n",
        "Personal Information that should be filled into code:\n",
        "domain: staging.md.ai\n",
        "personal token: ClarenceToken\n",
        "project id: x9N2KMqZ\n",
        "dataset id: D_2Nn7dG\n",
        "label id: x8dfojief\n",
        "Dicom SR output folder: ./SR_Output\n",
        "Dataframe output folder: ./Clarence_folder\n",
        "model id: M_Ll9Q0l\n",
        "\n",
        "\"\"\"\n",
        "report_url = \"\"\"\n",
        "use this link\n",
        "https://docs.md.ai/libraries/python/guides\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_prompt = report_url + report_fixed_part + prompt"
      ],
      "metadata": {
        "id": "-YATypPEiq6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo2: export annotation to dicom sr\n",
        "\n"
      ],
      "metadata": {
        "id": "uYuMA4kg3zwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"Question: Write me a script that works for the whole pipeline of downloading MD.ai annotations and exporting them into DICOM SR format to local folder\"\n",
        "report_fixed_part = \"\"\"You are an intelligent assistant helping users with their questions on techniques of MDai cloud platform.\n",
        "There is a set of rules:\n",
        "1. If the context does not answer some parts, and these parts are about coding, use common coding knowledge.\n",
        "2. For code that interacts with MD.ai, strictly follow the function names\n",
        "3. For conceptual questions, do not make up an answer and strictly answer based on the context. If the context is empty, just say \"I do not know the answer.”\n",
        "4. Some of the personal information might not be needed\n",
        "5.After downloading annotation json file, always use ”glob” library to find name. Do not create a random name of the file. Key words to use: \"annotation\", \"dicom_metadata\", \"model_outputs\"\n",
        "6.mdai has a utils module and contains library like common_utils\n",
        "7. MDai download functions mdai_client.project and mdai_client.download_dicom_metadata functions have an optional parameter called \"dataset_id\" , they have to match, such as both = 1234 or both = None.\n",
        "8. Start with \"!pip install\" the library that is not in Google Colab notebook by default\n",
        "9. All generated code will be upload to gist and then output as a colab notebook link\n",
        "10. The mdai authentication function is mdai.Client(domain, access_token), do not do any extra authentication step.\n",
        "\n",
        "tips:\n",
        "The pipeline of exporting dicom sr is 1. download the annotation and metadata json files using the mdai_client.project and mdai_client.download_dicom_metadata functions. Do not download anything else such as model output, do not convert json to dataframe 2. after downloading, find these 2 json files by keyword 'dicom_metadata' and 'annotations', pass them as input to do the DICOM SR export\n",
        "\n",
        "The pipeline of creating label is: download annotations json, convert json to dataframe, create the mapping dictionary\n",
        "\n",
        "For convert Dicom SR to annotations, assume there is already an output folder that contains SR file (so do not download for these tasks) and iterate all files ending with dcm. Assume the SR_to_Annot function is already in library mdai.utils.common_utils\n",
        "\n",
        "Personal Information that should be filled into code:\n",
        "domain: staging.md.ai\n",
        "personal token: ClarenceToken\n",
        "project id: x9N2KMqZ\n",
        "dataset id: D_2Nn7dG\n",
        "label id: x8dfojief\n",
        "Dicom SR output folder: ./SR_Output\n",
        "Dataframe output folder: ./Clarence_folder\n",
        "model id: M_Ll9Q0l\n",
        "\n",
        "\"\"\"\n",
        "report_url2 = \"\"\"\n",
        "use this link\n",
        "https://docs.md.ai/libraries/python/guides/\n",
        "https://github.com/ClarenceJiang71/mdai_place/blob/main/SR_seg_library_dist.md\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_prompt2 = report_url2 + report_fixed_part + prompt2"
      ],
      "metadata": {
        "id": "1zmjElUf5LAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo3: creating labels for each label id\n",
        "\n"
      ],
      "metadata": {
        "id": "tFeIqizs-KQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = \"Question: write me a script that works for the whole pipeline of creating labels for each label id\"\n",
        "report_fixed_part = \"\"\"You are an intelligent assistant helping users with their questions on techniques of MDai cloud platform.\n",
        "There is a set of rules:\n",
        "1. If the context does not answer some parts, and these parts are about coding, use common coding knowledge.\n",
        "2. For code that interacts with MD.ai, strictly follow the function names\n",
        "3. For conceptual questions, do not make up an answer and strictly answer based on the context. If the context is empty, just say \"I do not know the answer.”\n",
        "4. Some of the personal information might not be needed\n",
        "5.After downloading annotation json file, always use ”glob” library to find name. Do not create a random name of the file. Key words to use: \"annotation\", \"dicom_metadata\", \"model_outputs\"\n",
        "6.mdai has a utils module and contains library like common_utils\n",
        "7. MDai download functions mdai_client.project and mdai_client.download_dicom_metadata functions have an optional parameter called \"dataset_id\" , they have to match, such as both = 1234 or both = None.\n",
        "8. Start with \"!pip install\" the library that is not in Google Colab notebook by default\n",
        "9. All generated code will be upload to gist and then output as a colab notebook link\n",
        "10. The mdai authentication function is mdai.Client(domain, access_token), do not do any extra authentication step.\n",
        "\n",
        "tips:\n",
        "The pipeline of exporting dicom sr is 1. download the annotation and metadata json files using the mdai_client.project and mdai_client.download_dicom_metadata functions. Do not download anything else such as model output, do not convert json to dataframe 2. after downloading, find these 2 json files by keyword 'dicom_metadata' and 'annotations', pass them as input to do the DICOM SR export\n",
        "\n",
        "The pipeline of creating label is: download annotations json, convert json to dataframe, create the mapping dictionary\n",
        "\n",
        "For convert Dicom SR to annotations, assume there is already an output folder that contains SR file (so do not download for these tasks) and iterate all files ending with dcm. Assume the SR_to_Annot function is already in library mdai.utils.common_utils\n",
        "\n",
        "Personal Information that should be filled into code:\n",
        "domain: staging.md.ai\n",
        "personal token: ClarenceToken\n",
        "project id: x9N2KMqZ\n",
        "dataset id: D_2Nn7dG\n",
        "label id: x8dfojief\n",
        "Dicom SR output folder: ./SR_Output\n",
        "Dataframe output folder: ./Clarence_folder\n",
        "model id: M_Ll9Q0l\n",
        "\n",
        "\"\"\"\n",
        "report_url3 = \"\"\"\n",
        "use this link\n",
        "https://docs.md.ai/libraries/python/guides-import/\n",
        "https://docs.md.ai/libraries/python/guides\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_prompt3 = report_url3 + report_fixed_part + prompt3"
      ],
      "metadata": {
        "id": "u-YTlCjh-UO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo4: convert Dicom SR to annotations\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLeJ_iEHuO8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt4 = \"Question: Write me the script that works for the whole pipeline that converts a SR dicom file to annotations\"\n",
        "report_fixed_part = \"\"\"You are an intelligent assistant helping users with their questions on techniques of MDai cloud platform.\n",
        "There is a set of rules:\n",
        "1. If the context does not answer some parts, and these parts are about coding, use common coding knowledge.\n",
        "2. For code that interacts with MD.ai, strictly follow the function names\n",
        "3. For conceptual questions, do not make up an answer and strictly answer based on the context. If the context is empty, just say \"I do not know the answer.”\n",
        "4. Some of the personal information might not be needed\n",
        "5.After downloading annotation json file, always use ”glob” library to find name. Do not create a random name of the file. Key words to use: \"annotation\", \"dicom_metadata\", \"model_outputs\"\n",
        "6.mdai has a utils module and contains library like common_utils\n",
        "7. MDai download functions mdai_client.project and mdai_client.download_dicom_metadata functions have an optional parameter called \"dataset_id\" , they have to match, such as both = 1234 or both = None.\n",
        "8. Start with \"!pip install\" the library that is not in Google Colab notebook by default\n",
        "9. All generated code will be upload to gist and then output as a colab notebook link\n",
        "10. The mdai authentication function is mdai.Client(domain, access_token), do not do any extra authentication step.\n",
        "\n",
        "tips:\n",
        "The pipeline of exporting dicom sr is 1. download the annotation and metadata json files using the mdai_client.project and mdai_client.download_dicom_metadata functions. Do not download anything else such as model output, do not convert json to dataframe 2. after downloading, find these 2 json files by keyword 'dicom_metadata' and 'annotations', pass them as input to do the DICOM SR export\n",
        "\n",
        "The pipeline of creating label is: download annotations json, convert json to dataframe, create the mapping dictionary\n",
        "\n",
        "For convert Dicom SR to annotations, assume there is already an output folder that contains SR file (so do not download for these tasks) and iterate all files ending with dcm. Assume the SR_to_Annot function is already in library mdai.utils.common_utils\n",
        "\n",
        "Personal Information that should be filled into code:\n",
        "domain: staging.md.ai\n",
        "personal token: ClarenceToken\n",
        "project id: x9N2KMqZ\n",
        "dataset id: D_2Nn7dG\n",
        "label id: x8dfojief\n",
        "Dicom SR output folder: ./SR_Output\n",
        "Dataframe output folder: ./Clarence_folder\n",
        "model id: M_Ll9Q0l\n",
        "\n",
        "\"\"\"\n",
        "report_url4 = \"\"\"\n",
        "use this link\n",
        "https://github.com/ClarenceJiang71/mdai_place/blob/main/SR_to_annotations_final.md\n",
        "\"\"\"\n",
        "\n",
        "final_prompt4 = report_url4 + report_fixed_part + prompt4"
      ],
      "metadata": {
        "id": "9fXF8yMwuOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Generates Code Response (no need to run it, just include it to make sure the generated codes are correct)"
      ],
      "metadata": {
        "id": "1k3u2fF2wn1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "          {\"role\": \"user\", \"content\": final_prompt3}]\n",
        "result = mdai_client.chat_completion.create(messages, model='gpt-4', temperature=0)\n",
        "response = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "n26vvkaKxXlN",
        "outputId": "faac5b43-942b-4c11-9690-7fb0dfcda38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, here is a Python script that follows the pipeline of creating labels for each label id:\\n\\n```python\\n!pip install mdai\\n\\nimport mdai\\nimport glob\\nimport pandas as pd\\n\\n# Personal Information\\nDOMAIN = 'staging.md.ai'\\nPERSONAL_TOKEN = 'ClarenceToken'\\nPROJECT_ID = 'x9N2KMqZ'\\nDATASET_ID = 'D_2Nn7dG'\\nOUTPUT_FOLDER = './Clarence_folder'\\n\\n# Instantiate the MD.ai client\\nmdai_client = mdai.Client(domain=DOMAIN, access_token=PERSONAL_TOKEN)\\n\\n# Download annotations json\\np = mdai_client.project(PROJECT_ID, path=OUTPUT_FOLDER, dataset_id=DATASET_ID, annotations_only=True)\\n\\n# Find the downloaded annotations json file\\nannotations_json_file = glob.glob(f'{OUTPUT_FOLDER}/*annotations*.json')[0]\\n\\n# Convert json to dataframe\\nresults = mdai.common_utils.json_to_dataframe(annotations_json_file)\\nlabels_df = results['labels']\\n\\n# Create the mapping dictionary\\nlabels_dict = dict(zip(labels_df.labelId, labels_df.labelName))\\n\\nprint(labels_dict)\\n```\\n\\nThis script first installs the `mdai` library, then it sets up the MD.ai client with your personal information. It downloads the annotations json file for the specified project and dataset. Then it finds the downloaded annotations json file in the specified output folder. It converts the json file to a dataframe and creates a dictionary mapping each label id to its corresponding label name. Finally, it prints out the created dictionary.\\n\\nPlease note that this script is based on the content from the provided links: [https://docs.md.ai/libraries/python/guides-import/](https://docs.md.ai/libraries/python/guides-import/) and [https://docs.md.ai/libraries/python/guides](https://docs.md.ai/libraries/python/guides).\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "qCO2euAUPva-",
        "outputId": "0d9720c1-11e7-4fc9-cacf-1667f918d956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nuse this link\\nhttps://docs.md.ai/libraries/python/guides\\n\\nYou are an intelligent assistant helping users with their questions on techniques of MDai cloud platform.\\nThere is a set of rules:\\n1. If the context does not answer some parts, and these parts are about coding, use common coding knowledge.\\n2. For code that interacts with MD.ai, strictly follow the function names\\n3. For conceptual questions, do not make up an answer and strictly answer based on the context. If the context is empty, just say \"I do not know the answer.”\\n4. Some of the personal information might not be needed\\n5.After downloading annotation json file, always use ”glob” library to find name. Do not create a random name of the file. Key words to use: \"annotation\", \"dicom_metadata\", \"model_outputs\"\\n6.mdai has a utils module and contains library like common_utils\\n7. MDai download functions mdai_client.project and mdai_client.download_dicom_metadata functions have an optional parameter called \"dataset_id\" , they have to match, such as both = 1234 or both = None.\\n8. Generate code that fits google colab, if there is any libray not built-in google colab environment by default, show the pip install of any required package first before showing any actual import code\\n\\ntips:\\nThe pipeline of exporting dicom sr is 1. download the annotation and metadata json files using the mdai_client.project and mdai_client.download_dicom_metadata functions. Do not download anything else such as model output, do not convert json to dataframe 2. after downloading, find these 2 json files by keyword \\'dicom_metadata\\' and \\'annotations\\', pass them as input to do the DICOM SR export\\n\\nThe pipeline of creating label is: download annotations json, convert json to dataframe, create the mapping dictionary\\n\\nPersonal Information that should be filled into code\\n=============\\ndomain: staging.md.ai\\npersonal token: ClarenceToken\\nproject id: x9N2KMqZ\\ndataset id: D_2Nn7dG\\nDicom SR output folder: ./SR_Output\\nDataframe output folder: ./Clarence_folder\\nmodel id: M_Ll9Q0l\\n=============\\n\\nQuestion: write me a script for the whole pipeline that gets a dataframe for annotation and store in my default folder'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize of the LLM response"
      ],
      "metadata": {
        "id": "n6MZ3u4Zx_oR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here is a Python script that follows the pipeline to download the annotation data from the MD.ai platform, convert it to a dataframe, and store it in your specified folder.\n",
        "\n",
        "```python\n",
        "# First, install the necessary libraries\n",
        "!pip install mdai\n",
        "!pip install pandas\n",
        "\n",
        "# Import the necessary libraries\n",
        "import mdai\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Define your personal information\n",
        "DOMAIN = 'http://staging.md.ai'\n",
        "YOUR_PERSONAL_TOKEN = 'ClarenceToken'\n",
        "PROJECT_ID = 'x9N2KMqZ'\n",
        "DATASET_ID = 'D_2Nn7dG'\n",
        "OUTPUT_FOLDER = './Clarence_folder'\n",
        "\n",
        "# Authenticate with MD.ai\n",
        "mdai_client = mdai.Client(domain=DOMAIN, access_token=YOUR_PERSONAL_TOKEN)\n",
        "\n",
        "# Download the annotation data\n",
        "p = mdai_client.project(PROJECT_ID, path=OUTPUT_FOLDER, annotations_only=True, dataset_id=DATASET_ID)\n",
        "\n",
        "# Find the downloaded annotation json file\n",
        "annotation_file = glob.glob(OUTPUT_FOLDER + '/*annotations*.json')[0]\n",
        "\n",
        "# Convert the json file to a dataframe\n",
        "results = mdai.common_utils.json_to_dataframe(annotation_file)\n",
        "annotations_df = results['annotations']\n",
        "\n",
        "# Save the dataframe to a csv file in your specified folder\n",
        "annotations_df.to_csv(OUTPUT_FOLDER + '/annotations.csv', index=False)\n",
        "\n",
        "print(\"The annotations dataframe has been saved to your specified folder.\")\n",
        "```\n",
        "\n",
        "Please replace the DOMAIN, YOUR_PERSONAL_TOKEN, PROJECT_ID, DATASET_ID, and OUTPUT_FOLDER with your actual information. This script will save the annotations dataframe as a CSV file in your specified folder."
      ],
      "metadata": {
        "id": "MIR6D--7W_vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Gist"
      ],
      "metadata": {
        "id": "3vDlUciPxVhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def find_python(response):\n",
        "  position = response.index(\"```python\")\n",
        "  start = position + len(\"```python\") + 1 # +1 to skip the \\n\n",
        "  end = response.rfind(\"```\")\n",
        "  return response[start:end]\n",
        "\n",
        "def convert_notebook(code):\n",
        "  cell_content = {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\" : 1,\n",
        "      \"metadata\": {},\n",
        "      \"outputs\": [],\n",
        "      \"source\": code\n",
        "  }\n",
        "\n",
        "  notebook_content = {\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "     cell_content\n",
        "    ]\n",
        "  }\n",
        "\n",
        "  final_content_json = json.dumps(notebook_content)\n",
        "  return final_content_json\n",
        "\n",
        "\n",
        "def upload_gist(gist_description, file_name, response):\n",
        "  # Replace these 2 with own tokens and git_id!\n",
        "  API_TOKEN = # your github API token\n",
        "  git_id = # your git id\n",
        "\n",
        "  # code = find_python(response)\n",
        "  # check if only using this convert notebook method\n",
        "  code = response\n",
        "  # print(code)\n",
        "  code_content = convert_notebook(code)\n",
        "  # code_content = response\n",
        "\n",
        "  GITHUB_API=\"https://api.github.com\"\n",
        "  url=GITHUB_API+\"/gists\"\n",
        "  headers={\n",
        "    'Authorization':'token {}'.format(API_TOKEN)\n",
        "  }\n",
        "  params={\n",
        "      'scope':'gist'\n",
        "  }\n",
        "  payload={\n",
        "          \"description\": gist_description,\n",
        "          \"public\":True,\n",
        "          \"files\":\n",
        "          {\n",
        "              file_name:\n",
        "              {\n",
        "                  \"content\": code_content\n",
        "              }\n",
        "          }\n",
        "  }\n",
        "\n",
        "  #make a post request\n",
        "  res=requests.post(url,headers=headers,params=params,data=json.dumps(payload))\n",
        "\n",
        "  #print response and also convert into JSON format\n",
        "  json_content=json.loads(res.text)\n",
        "\n",
        "  json_formatted_str = json.dumps(json_content, indent=4)\n",
        "\n",
        "  id = json_content['id']\n",
        "  url = \"https://colab.research.google.com/gist/\"+git_id+\"/\"+id\n",
        "  return url\n"
      ],
      "metadata": {
        "id": "YVfdtbMaXml_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gist_descrpition = \"LLM generated code\"\n",
        "# file_name = \"LLM_download_annotation_csv.ipynb\"\n",
        "# # code_content = find_python(response)\n",
        "# Run of the upload_gist function, this is not a test driven by LLM natural language, but rather triggered by manually running the function\n",
        "# colab_url = upload_gist(gist_descrpition, file_name, response)\n",
        "# colab_url"
      ],
      "metadata": {
        "id": "NXWB8Mrms5MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test function calling, the final function to use is called \"run_conversation\"."
      ],
      "metadata": {
        "id": "o2NG9J8hmiRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import json\n",
        "def run_conversation(final_prompt):\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "    messages = [{\"role\": \"user\", \"content\": final_prompt}]\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"upload_gist\",\n",
        "            \"description\": \"upload a code snippet, which represents a pipeline, into gist and then return a publicly accessible colab notebook url\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"gist_description\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The description of the gist, e.g. LLM-generated code for downloading annoatation and convert to dataframe\",\n",
        "                    },\n",
        "                    \"file_name\": {\"type\": \"string\", \"description\": \"The name of the gist, have to end with ipynb, e.g. LLM_convert_annotation_to_dataframe.ipynb\"},\n",
        "                    \"response\" : {\"type\" : \"string\", \"description\": \"The response generated by LLM, it involve some code, when change to new line, make sure it is \\\\n, so it matches json format\"}\n",
        "                },\n",
        "                \"required\": [\"gist_description\", \"file_name\", 'response'],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    response = mdai_client.chat_completion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "    # print(response_message)\n",
        "\n",
        "    \"\"\"\n",
        "    At this step there is no content, function call and arguments are set up\n",
        "    If the question is not related to the function call, it will not have a\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"upload_gist\": upload_gist,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        function_name = response_message[\"function_call\"][\"name\"]\n",
        "        fuction_to_call = available_functions[function_name]\n",
        "\n",
        "\n",
        "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "        function_response = fuction_to_call(\n",
        "            # This seens just hard code\n",
        "            gist_description=function_args.get(\"gist_description\"),\n",
        "            file_name=function_args.get(\"file_name\"),\n",
        "            response =function_args.get(\"response\"),\n",
        "        )\n",
        "\n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "        second_response = mdai_client.chat_completion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "        return second_response\n",
        "\n",
        "\n",
        "\n",
        "# response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "iRyK57Q1mlTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo1 asks for download annotations then convert to dataframe and store as csv in a given place\n",
        "response = run_conversation(final_prompt)\n",
        "response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uqXwR3nfXhn6",
        "outputId": "67b88d0a-3d3d-4a98-8da9-3123efa20eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/c6fb40d1efd37737c679a020fbd092ee) to the Colab notebook with the script for downloading annotations and converting them to a dataframe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/c6fb40d1efd37737c679a020fbd092ee) to the Colab notebook with the script for downloading annotations and converting them to a dataframe."
      ],
      "metadata": {
        "id": "UCCGPlKhYBEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo2: export annotation to dicom sr\n",
        "response2 = run_conversation(final_prompt2)\n",
        "response2['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "RauifyTUXvvh",
        "outputId": "7f194f22-350b-44aa-ee46-03eb269c13d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/d13487895dccd5167cbb99e226e017e5) to the Colab notebook with the script for the whole pipeline of downloading MD.ai annotations and exporting them into DICOM SR format to a local folder.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/d13487895dccd5167cbb99e226e017e5) to the Colab notebook with the script for the whole pipeline of downloading MD.ai annotations and exporting them into DICOM SR format to a local folder."
      ],
      "metadata": {
        "id": "CxpS95t5Yz4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo3: creating labels for each label id\n",
        "response3 = run_conversation(final_prompt3)\n",
        "response3['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4sI1iOnFY07H",
        "outputId": "f2d87913-7ced-4f87-b2e5-dd639029eac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/9ac9dcef6a4a1ab3426b101ec2aeca05) to the Colab notebook with the script for creating labels for each label id.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the [link](https://colab.research.google.com/gist/ClarenceJiang71/9ac9dcef6a4a1ab3426b101ec2aeca05) to the Colab notebook with the script for creating labels for each label id."
      ],
      "metadata": {
        "id": "Hlx2UAw-aHlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo4: convert dicom sr to annotation\n",
        "response4 = run_conversation(final_prompt4)\n",
        "response4['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CwEVqkBEUhD3",
        "outputId": "0d5eda7a-1ab7-47f4-83b7-1d9e88b49b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is the script that converts a SR dicom file to annotations: [SR_to_Annotations.ipynb](https://colab.research.google.com/gist/ClarenceJiang71/a8669aae147c0418e1be145c7aeefd1b)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the script that converts a SR dicom file to annotations: [SR_to_Annotations.ipynb](https://colab.research.google.com/gist/ClarenceJiang71/a8669aae147c0418e1be145c7aeefd1b)"
      ],
      "metadata": {
        "id": "Z_JkZvBtXNUH"
      }
    }
  ]
}